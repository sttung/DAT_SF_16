{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(data.phoneNo_incl, data.noTimesContactAttempted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploration of our Number of Times Contact Attempted & Number of Times Reached variables' missing values. \n",
    "\n",
    "# Exploration: is our data on the number of times contact was attempted randomly missing?\n",
    "# & is data on the number of times someone was reached randomly missing?\n",
    "# How to check: are there statistically significant differences between features, if we split our data on missing/non-missing?\n",
    "\n",
    "# (1) Number of Times Contact Attempted feature: \n",
    "# Create dummies to separate out noTimesContactAttempted missing / non-missing observations\n",
    "model2c = data\n",
    "model2c['noTimesContAtt_incl'] = data.noTimesContactAttempted.notnull()==True\n",
    "model2c['noTimesReached_incl'] = data.noTimesReached.notnull()==True\n",
    "# model2c.head()\n",
    "\n",
    "# Import ttest from SciPy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define the 2 groups to be compared for number of times contact attempted\n",
    "noTCA_1 = model2c[model2c['noTimesContAtt_incl']==True]\n",
    "noTCA_2 = model2c[model2c['noTimesContAtt_incl']==False]\n",
    "\n",
    "# Run a t-test on verified (even though it's a dummy, we can look at the means to gauge if more reports were verif or not)\n",
    "\n",
    "# Define a function that runs ttests on a couple different features to identify any stat signif differences\n",
    "def tt_feat1(features):\n",
    "    print\n",
    "    print \"t-test results for number of times contact attempted\"\n",
    "    # Loop through the list of features we want to test for stat signif differences between\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTCA_1[x], noTCA_2[x])\n",
    "        \n",
    "tt_feat1(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])      \n",
    "\n",
    "# (2) Number of Times Contact Reached: \n",
    "# Define the 2 groups to be compared for number of times reached\n",
    "noTR_1 = model2c[model2c['noTimesReached_incl']==True]\n",
    "noTR_2 = model2c[model2c['noTimesReached_incl']==False]\n",
    "\n",
    "def tt_feat2(features):\n",
    "    print\n",
    "    print \"t-test results for number of times reached:\"\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTR_1[x], noTR_2[x])    \n",
    "        \n",
    "tt_feat2(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])\n",
    "\n",
    "# Next steps:\n",
    "# Use average\n",
    "# then can try distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO UPDATE: CREATE A CLASS SO THAT BALANCED_M1DATA CAN BE MODIFIED AND USED BY THE BAL_MODELLR FUNCTION! \n",
    "# def b_dataset:\n",
    "#     # Note that we should have 1634 observations for this set.\n",
    "#     neg = model1_data[model1_data['actionTaken'] == 0]\n",
    "#     print neg.info()\n",
    "\n",
    "#     # Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "#     neg_samp = neg.sample(401)\n",
    "\n",
    "#     # Construct the positive (action taken on reports) subsample. \n",
    "#     pos = model1_data[model1_data['actionTaken'] == 1]\n",
    "\n",
    "#     # Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "#     balanced_m1data = pd.concat([neg_samp,pos])\n",
    "#     balanced_m1data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for the create sample segment of our class - updated to add a parameter, n, that allows us to vary the size\n",
    "#     def create_sample(self):\n",
    "#         self.pos = self.data_train[self.data_train['actionTaken'] ==1]\n",
    "#         print \n",
    "#         print 'action taken observations: ', len(self.pos)\n",
    "#         # Create set with only negative (no action taken) observations\n",
    "#         self.neg = self.data_train[self.data_train['actionTaken'] == 0]\n",
    "#         print 'no action taken (NAT) observations: ', len(self.neg)\n",
    "#         print \n",
    "#         # Undersample negative cases that match the # positive cases we have\n",
    "#         self.neg_samp = self.neg.sample(len(self.pos))\n",
    "#         print 'sampled NAT observations: ', len(self.neg_samp)\n",
    "#         print   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is old code used to create a training dataset. This was re-ordered so that data would be split\n",
    "# more correctly. \n",
    "\n",
    "# Note that only about 20% of our reports had action taken on them. \n",
    "# To make sure that our model can accurately predict reports where action was taken, we want to create a more balanced dataset.\n",
    "# We create one that is 50/50 \n",
    "\n",
    "# As a first step - separate out the data into cases where reports had action taken (a/t), or no a/t. \n",
    "\n",
    "# Create a variable that stores cases where action is not taken on a report.\n",
    "# Note that we should have 1634 observations for this set.\n",
    "neg = model1_data[model1_data['actionTaken'] == 0]\n",
    "neg.info()\n",
    "\n",
    "# Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "neg_samp = neg.sample(401)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos = model1_data[model1_data['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m1data = pd.concat([neg_samp,pos])\n",
    "balanced_m1data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logistic regression model\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Create a function to test out our features using a logistic regression model.\n",
    "# This will allow us to more quickly add features and trims the amount of code we need to use each time. \n",
    "def bal_modelLR(baldf, modeldf, model):\n",
    "    # Create our training & test datasets \n",
    "    for train,test in ShuffleSplit(len(baldf), n_iter=1, test_size=0.25,random_state=0):\n",
    "        # Drop label from training data\n",
    "        data_train = baldf.iloc[train].drop(['actionTaken'], axis=1)\n",
    "        # Create label set \n",
    "        label_train = baldf['actionTaken'].iloc[train]\n",
    "        # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "        data_test = modeldf.iloc[test].drop(['actionTaken'], axis=1)\n",
    "        label_test = modeldf['actionTaken'].iloc[test]\n",
    "\n",
    "    # Run logistic regression on our training data\n",
    "    model_lr = model.fit(data_train, label_train)\n",
    "\n",
    "    # Print out details on precision, recall for our model. \n",
    "    print classification_report(label_test,model_lr.predict(data_test))\n",
    "    \n",
    "    # Print out the coefficients\n",
    "    model.coef[0]\n",
    "    print zip(model.coef_[0],modeldf.drop('actionTaken',axis=1))\n",
    "\n",
    "bal_modelLR(balanced_m1data, model1_data,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is old code used to run the first logistic regression model.\n",
    "# Replaced this with a function, because we're essentially running the same code each time we build a model.\n",
    "# Using a function makes the code much easier to follow as long as the function is well-explained / commented. \n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m1data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train = balanced_m1data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train = balanced_m1data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test = model1_data.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test = model1_data['actionTaken'].iloc[test]\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr1 = model.fit(data_train, label_train)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test,model_lr1.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All old code used for first logistic regression model\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(model1_data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Create training data + label - don't drop label yet. \n",
    "    data_train = model1_data.iloc[train]\n",
    "    # Define our test data (features), label. \n",
    "    data_test = model1_data.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test = model1_data['actionTaken'].iloc[test]\n",
    "\n",
    "print 'data_train info'\n",
    "print data_train.info()  \n",
    "print \n",
    "print 'data_test info'\n",
    "print data_test.info()\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos = data_train[data_train['actionTaken'] == 1]\n",
    "print 'pos samp info'\n",
    "print pos.info()\n",
    "print \n",
    "\n",
    "# Create set with only negative (no action taken) observations\n",
    "neg = data_train[data_train['actionTaken'] == 0]\n",
    "print 'neg info'\n",
    "print neg.info()\n",
    "print \n",
    "\n",
    "# Sample 317 observations of this subsample. We select 317 to match the number of reports where action was taken. \n",
    "neg_samp = neg.sample(317)\n",
    "print 'neg sample info'\n",
    "print neg_samp.info()\n",
    "print \n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "bal_m1data = pd.concat([neg_samp,pos])\n",
    "train_data = bal_m1data.drop(['actionTaken'],axis=1)\n",
    "train_label = bal_m1data['actionTaken']\n",
    "print 'train_data info'\n",
    "print train_data.info()\n",
    "print\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr1 = model.fit(train_data, train_label)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test,model_lr1.predict(data_test))\n",
    "\n",
    "# Results from this, using shufflesplit, got us around 31-32% precision and around 76% recall. Not as good\n",
    "# and why we used stratified shuffle split! \n",
    "\n",
    "# Zip features and coefficients together\n",
    "sorted(zip(model_lr1.coef_[0],model1_data.drop('actionTaken',axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for plotting feature importance for model 1\n",
    "# Plot Feature Importance, based on Model 1\n",
    "\n",
    "# Define features\n",
    "features = model1_data[['verified', 'actionable', 'urgent', 'phoneNo_incl', 'locAccurFixer_incl']]\n",
    "# Define target\n",
    "target = model1_data[['actionTaken']]\n",
    "\n",
    "# Set x's range to be the number of features\n",
    "x = np.arange(len(features.columns))\n",
    "\n",
    "# Set names for each column to be the feature names\n",
    "names = features.columns\n",
    "names\n",
    "\n",
    "# Plot feature importance, as measured by coefficients on each of the features\n",
    "p = figure(title=\"Model Coefficients\")\n",
    "\n",
    "for val in x:\n",
    "    p.quad(top=model_lr1.coef_.ravel()[val], \n",
    "           bottom=0, left=val+0.2,right=val+0.8, \n",
    "           color=['red','orange','yellow', 'green', 'blue', 'purple', 'brown','pink','black'][val],\n",
    "           legend=names[val])\n",
    "# Set the range for y based on minimum and max values of the feature coefficients\n",
    "p.y_range = Range1d(min(model_lr1.coef_.ravel()), max(model_lr1.coef_.ravel()))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for model 2. \n",
    "\n",
    "# Old code for dropping out extraneous variables\n",
    "# Drop extraneous variables\n",
    "# Create a list with the name of variables we aren't using. Reference in the future to avoid listing out all the names again.\n",
    "extr_var = ['caseID','incidentTitle','incidentDate','location','desc','category',\\\n",
    "             'latitude','longitude','phoneNo','mostAffctDistr','locAccuracy','forApproval',\\\n",
    "                  'routedStatusOrg','noTimesRouted','locAccurFixer', 'dispatchStatus_orgagreed',\\\n",
    "                  'dispatchOrgCapacity','firstName','lastName','email','closed',\\\n",
    "                  'actionSumm','comment','commentDate', 'approved','noTimesContactAttempted',\\\n",
    "                           'noTimesReached', 'incTitle_len', 'desc_len', 'urgent','phoneNo_incl']\n",
    "\n",
    "# Old code - version 1 for creating dataframe for model 2\n",
    "\n",
    "# Prep the data for model 2\n",
    "neg2 = model2a[model2a['actionTaken'] == 0]\n",
    "print \"Model 2: Neg subsample\"\n",
    "neg2.info()\n",
    "print \n",
    "\n",
    "# Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "neg_samp2 = neg2.sample(401)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos2 = model2a[model2a['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m2data = pd.concat([neg_samp2,pos2])\n",
    "print \"Model 2: Sampled Dataset\"\n",
    "balanced_m2data.info()\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m2data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train2 = balanced_m2data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train2 = balanced_m2data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test2 = model2a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test2 = model2a['actionTaken'].iloc[test]\n",
    "\n",
    "\n",
    "    \n",
    "# Run logistic regression on our training data\n",
    "model_lr2 = model2.fit(data_train2, label_train2)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test2,model_lr2.predict(data_test2))\n",
    "\n",
    "# Zip features and coefficients together\n",
    "pd.DataFrame(sorted(zip(model1a_data.drop('actionTaken',axis=1),model_lr1a.coef_[0])),columns=['Feature','coefficient'])\n",
    "\n",
    "\n",
    "# Feature Importance, based on Model 1\n",
    "\n",
    "# Define features\n",
    "features = model1a_data[['verified', 'actionable', 'phoneNo_incl', 'locAccurFixer_incl']]\n",
    "# Define target\n",
    "target = model1a_data[['actionTaken']]\n",
    "\n",
    "# Set x's range to be the number of features\n",
    "x = np.arange(len(features.columns))\n",
    "\n",
    "# Set names for each column to be the feature names\n",
    "names = features.columns\n",
    "names\n",
    "\n",
    "# Plot feature importance, as measured by coefficients on each of the features\n",
    "p = figure(title=\"Model Coefficients\")\n",
    "\n",
    "for val in x:\n",
    "    p.quad(top=model_lr1a.coef_.ravel()[val], \n",
    "           bottom=0, left=val+0.2,right=val+0.8, \n",
    "           color=['red','orange','yellow', 'green', 'blue', 'purple', 'brown','pink','black'][val],\n",
    "           legend=names[val])\n",
    "# Set the range for y based on minimum and max values of the feature coefficients\n",
    "p.y_range = Range1d(min(model_lr1a.coef_.ravel()), max(model_lr1a.coef_.ravel()))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for model 2 - version 2 with correct order of split, balance\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(model2a), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Create training data + label - don't drop label yet. \n",
    "    data2_train = model2a.iloc[train]\n",
    "    # Define our test data (features), label. \n",
    "    data2_test = model2a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label2_test = model2a['actionTaken'].iloc[test]\n",
    "\n",
    "print 'data_train info'\n",
    "print data2_train.info()  \n",
    "print \n",
    "print 'data_test info'\n",
    "print data2_test.info()\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos2 = data2_train[data2_train['actionTaken'] == 1]\n",
    "print 'pos samp info'\n",
    "print pos2.info()\n",
    "print \n",
    "\n",
    "# Create set with only negative (no action taken) observations\n",
    "neg2 = data2_train[data2_train['actionTaken'] == 0]\n",
    "print 'neg info'\n",
    "print neg2.info()\n",
    "print \n",
    "\n",
    "# Sample 317 observations of this subsample. We select 317 to match the number of reports where action was taken. \n",
    "neg_samp2 = neg2.sample(317)\n",
    "print 'neg sample info'\n",
    "print neg_samp2.info()\n",
    "print \n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "bal_m2data = pd.concat([neg_samp2,pos2])\n",
    "train_data2 = bal_m2data.drop(['actionTaken'],axis=1)\n",
    "train_label2 = bal_m2data['actionTaken']\n",
    "print 'train_data info'\n",
    "print train_data.info()\n",
    "print\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr2 = LogisticRegression(C=1).fit(train_data2, train_label2)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label2_test,model_lr2.predict(data2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for model 3\n",
    "\n",
    "# Prep the data for model 3\n",
    "neg3 = model3a[model3a['actionTaken'] == 0]\n",
    "print \"Model 3: Neg subsample\"\n",
    "neg3.info()\n",
    "print \n",
    "\n",
    "# Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "neg_samp3 = neg3.sample(401)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos3 = model3a[model3a['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m3data = pd.concat([neg_samp3,pos3])\n",
    "print \"Model 3: Sampled Dataset\"\n",
    "balanced_m3data.info()\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m3data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train3 = balanced_m3data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train3 = balanced_m3data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test3 = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test3 = model3a['actionTaken'].iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for prepping data for models\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(model2a), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Create training data + label - don't drop label yet. \n",
    "    data2_train = model2a.iloc[train]\n",
    "    # Define our test data (features), label. \n",
    "    data2_test = model2a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label2_test = model2a['actionTaken'].iloc[test]\n",
    "\n",
    "print 'data_train info'\n",
    "print data2_train.info()  \n",
    "print \n",
    "print 'data_test info'\n",
    "print data2_test.info()\n",
    "\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos2 = data2_train[data2_train['actionTaken'] == 1]\n",
    "print 'pos samp info'\n",
    "print pos2.info()\n",
    "print \n",
    "\n",
    "\n",
    "# Create set with only negative (no action taken) observations\n",
    "neg2 = data2_train[data2_train['actionTaken'] == 0]\n",
    "print 'neg info'\n",
    "print neg2.info()\n",
    "print \n",
    "\n",
    "# Sample 317 observations of this subsample. We select 317 to match the number of reports where action was taken. \n",
    "neg_samp2 = neg2.sample(317)\n",
    "print 'neg sample info'\n",
    "print neg_samp2.info()\n",
    "print \n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "bal_m2data = pd.concat([neg_samp2,pos2])\n",
    "train_data2 = bal_m2data.drop(['actionTaken'],axis=1)\n",
    "train_label2 = bal_m2data['actionTaken']\n",
    "print 'train_data info'\n",
    "print train_data.info()\n",
    "print\n",
    "\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr2 = LogisticRegression(C=1).fit(train_data2, train_label2)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label2_test,model_lr2.predict(data2_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for shifting balance in our model\n",
    "\n",
    "# Part 1: \n",
    "# Balance: 20% negative, 80% positive\n",
    "# Prep the data for model 3\n",
    "neg3a = model3a[model3a['actionTaken'] == 0]\n",
    "print \"Model 3: Neg subsample\"\n",
    "neg3.info()\n",
    "print \n",
    "\n",
    "# Sample 100 observations of this subsample. We select 100 to create an 80/20 split of +/-.\n",
    "neg_samp3a = neg3a.sample(100)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos3a = model3a[model3a['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m3adata = pd.concat([neg_samp3a,pos3a])\n",
    "print \"Model 3: Sampled Dataset\"\n",
    "balanced_m3adata.info()\n",
    "\n",
    "\n",
    "# Part 2:\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m3adata), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train3 = balanced_m3adata.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train3 = balanced_m3adata['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test3 = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test3 = model3a['actionTaken'].iloc[test]\n",
    "    \n",
    "# Run logistic regression on our training data\n",
    "model_lr3 = LogisticRegression(C=1).fit(data_train3, label_train3)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test3,model_lr3.predict(data_test3))\n",
    "\n",
    "\n",
    "# Part 3: \n",
    "# Create & zip lists of % negative, precision, recall for 1\n",
    "neg_pct = [20, 30, 37, 50, 56, 75, 125, 149, 175]\n",
    "neg_no = [100, 130, 150, 200, 225, 300, 500, 600, 700]\n",
    "prec = [0.23, 0.29, 0.25, 0.32, 0.29, 0.30, 0.40, 0.53, 0.46]\n",
    "rec = [0.86, 0.93, 0.95, 0.87, 0.75, 0.88, 0.74, 0.83, 0.79]\n",
    "\n",
    "pd.DataFrame(zip(neg_pct, neg_no, prec, rec),columns=['Neg Percent','# Neg Samples','Precision', 'Recall'])\n",
    "\n",
    "\n",
    "# Part 4: \n",
    "# Balance: 30% negative, 80% positive\n",
    "# Prep the data for model 3\n",
    "neg3a = model3a[model3a['actionTaken'] == 0]\n",
    "print \"Model 3: Neg subsample\"\n",
    "neg3.info()\n",
    "print \n",
    "\n",
    "# Sample 130 observations of this subsample. We select 100 to create an 80/20 split of +/-.\n",
    "neg_samp3a = neg3a.sample(600)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos3a = model3a[model3a['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m3adata = pd.concat([neg_samp3a,pos3a])\n",
    "print \"Model 3: Sampled Dataset\"\n",
    "balanced_m3adata.info()\n",
    "\n",
    "\n",
    "# Part 5: \n",
    "for train,test in ShuffleSplit(len(balanced_m3adata), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train3 = balanced_m3adata.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train3 = balanced_m3adata['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test3 = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test3 = model3a['actionTaken'].iloc[test]\n",
    "    \n",
    "# Run logistic regression on our training data\n",
    "model_lr3 = LogisticRegression(C=1).fit(data_train3, label_train3)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test3,model_lr3.predict(data_test3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check feature importance for this model\n",
    "model.coef_[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 3 code\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(model3a), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Create training data + label - don't drop label yet. \n",
    "    data3_train = model3a.iloc[train]\n",
    "    # Define our test data (features), label. \n",
    "    data3_test = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label3_test = model3a['actionTaken'].iloc[test]\n",
    "\n",
    "print 'data_train info'\n",
    "print data3_train.info()  \n",
    "print \n",
    "print 'data_test info'\n",
    "print data3_test.info()\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos3 = data3_train[data3_train['actionTaken'] == 1]\n",
    "print 'pos samp info'\n",
    "print pos3.info()\n",
    "print \n",
    "\n",
    "# Create set with only negative (no action taken) observations\n",
    "neg3 = data3_train[data3_train['actionTaken'] == 0]\n",
    "print 'neg 3 info'\n",
    "print neg3.info()\n",
    "print \n",
    "\n",
    "# Sample 317 observations of this subsample. We select 317 to match the number of reports where action was taken. \n",
    "neg_samp3 = neg3.sample(317)\n",
    "print 'neg sample 3 info'\n",
    "print neg_samp3.info()\n",
    "print \n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "bal_m3data = pd.concat([neg_samp3,pos3])\n",
    "train_data3 = bal_m3data.drop(['actionTaken'],axis=1)\n",
    "train_label3 = bal_m3data['actionTaken']\n",
    "print 'train_data3 info'\n",
    "print train_data3.info()\n",
    "print\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr3 = LogisticRegression(C=1).fit(train_data3, train_label3)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label3_test,model_lr3.predict(data3_test))\n",
    "\n",
    "# Check feature importance for this model\n",
    "model_lr3.coef_[0]\n",
    "\n",
    "# Zip features and coefficients together\n",
    "sorted(zip(model_lr3.coef_[0],model3a.drop('actionTaken',axis=1)), reverse = True)\n",
    "\n",
    "# Feature Importance, based on Model 3\n",
    "\n",
    "# Define features\n",
    "features = model3a[['verified', 'actionable', 'urgent', 'phoneNo_incl', 'locAccurFixer_incl', 'incTitle_len',\\\n",
    "                  'desc_len','noTimesContAtt_imp','noTimesRea_imp']]\n",
    "# Define target\n",
    "target = model3a[['actionTaken']]\n",
    "\n",
    "# Set x's range to be the number of features\n",
    "x = np.arange(len(features.columns))\n",
    "\n",
    "# Set names for each column to be the feature names\n",
    "names = features.columns\n",
    "names\n",
    "\n",
    "model_lr3.coef_[0]\n",
    "\n",
    "sorted(zip(model_lr3.coef_[0], model3a.drop('actionTaken', axis = 1).columns.tolist()),reverse=True)\n",
    "\n",
    "# OH HELP: Fixing this \n",
    "\n",
    "# Plot feature importance, as measured by coefficients on each of the features\n",
    "p = figure(title=\"Model Coefficients\")\n",
    "\n",
    "for val in x:\n",
    "    p.quad(top=model_lr3.coef_.ravel()[val], \n",
    "           bottom=0, left=val+0.2,right=val+0.8, \n",
    "           color=['red','orange','yellow', 'green', 'blue', 'purple', 'brown','pink','black'][val],\n",
    "           legend=names[val])\n",
    "# Set the range for y based on minimum and max values of the feature coefficients\n",
    "p.y_range = Range1d(min(model_lr3.coef_.ravel()), max(model_lr3.coef_.ravel()))\n",
    "show(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for setting up data for re-balanced classes\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(model3a), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Create training data + label - don't drop label yet. \n",
    "    data3_train = model3a.iloc[train]\n",
    "    # Define our test data (features), label. \n",
    "    data3_test = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label3_test = model3a['actionTaken'].iloc[test]\n",
    "\n",
    "print 'data_train info'\n",
    "print data3_train.info()  \n",
    "print \n",
    "print 'data_test info'\n",
    "print data3_test.info()\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos3 = data3_train[data3_train['actionTaken'] == 1]\n",
    "print 'pos samp info'\n",
    "print pos3.info()\n",
    "print \n",
    "\n",
    "# Create set with only negative (no action taken) observations\n",
    "neg3 = data3_train[data3_train['actionTaken'] == 0]\n",
    "print 'neg 3 info'\n",
    "print neg3.info()\n",
    "print \n",
    "\n",
    "# Sample 317 observations of this subsample. We select 317 to match the number of reports where action was taken. \n",
    "neg_samp3 = neg3.sample(700)\n",
    "print 'neg sample 3 info'\n",
    "print neg_samp3.info()\n",
    "print \n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "bal_m3data = pd.concat([neg_samp3,pos3])\n",
    "train_data3 = bal_m3data.drop(['actionTaken'],axis=1)\n",
    "train_label3 = bal_m3data['actionTaken']\n",
    "print 'train_data3 info'\n",
    "print train_data3.info()\n",
    "print\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr3 = LogisticRegression(C=1).fit(train_data3, train_label3)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label3_test,model_lr3.predict(data3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing features: because we now have actual continuous variables & dummy variables, we need to normalize so they're\n",
    "# on the same scale. \n",
    "\n",
    "# Import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create dataframe with the variables we want to normalize\n",
    "model3_datatn = model3_data[['noTimesContAtt_imp','noTimesRea_imp','incTitle_len','desc_len',]]\n",
    "model3_datatn.head()\n",
    "\n",
    "# Normalize the data\n",
    "fit_data = StandardScaler().fit_transform(model3_datatn)\n",
    "model3_data_n = pd.DataFrame(fit_data, columns=model3_datatn.columns)\n",
    "model3_data_n.info()\n",
    "\n",
    "\n",
    "# Create & zip lists of % negative, precision, recall for 1\n",
    "neg_no = [400, 500, 600, 650, 700]\n",
    "prec = [0.41, 0.41, 0.42, 0.42, 0.43]\n",
    "rec = [0.73, 0.68, 0.63, 0.62, 0.56]\n",
    "f1 = [0.52, 0.51, 0.50, 0.50, 0.48]\n",
    "\n",
    "pd.DataFrame(zip(neg_no, prec, rec, f1),columns=['Neg Samples','Precision', 'Recall','F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for RFC\n",
    "\n",
    "# Test out a random forest classifier on the same data \n",
    "for train,test in ShuffleSplit(len(balanced_m3adata), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train3 = balanced_m3adata.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train3 = balanced_m3adata['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test3 = model3a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test3 = model3a['actionTaken'].iloc[test]\n",
    "\n",
    "# Import RFC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Set up rfc model\n",
    "rfc = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=9)\n",
    "rfc_m3 = rfc.fit(data_train3,label_train3)\n",
    "print \n",
    "print \n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test3,rfc_m3.predict(data_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for trying to deal with errors in the text analysis\n",
    "\n",
    "# One-off way to replace problematic non-UTF-8 characters - need a systematic way to clean these\n",
    "# Ideally, would decode > encode. Decode doesn't work.\n",
    "data.incidentTitle.str.decode('utf-8')\n",
    "\n",
    "# This is just a one-off way to replace the problematic characters. Need a systematic approach. \n",
    "data.incidentTitle.apply(lambda x: x.replace('\\x96', ''))\n",
    "\n",
    "# Just a one-off check to see what character was causing the problem \n",
    "data.incidentTitle.ix[1102][26]\n",
    "\n",
    "# Temporary fix for looping through and identifying when there's an error\n",
    "for idx, row in enumerate(data.incidentTitle.apply(lambda x: x.replace('\\x96', ''))):\n",
    "    print idx\n",
    "    unicode(row, \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def tfidf_inc_LR(self):\n",
    "        # create vectorizer object\n",
    "        vectorizer = TfidfVectorizer(decode_error='replace', stop_words='english', max_features=800)\n",
    "        \n",
    "        # convert documents and their labels into numpy arrays for training data\n",
    "        self.Xtis_train = vectorizer.fit_transform(self.train_data['incidentTitle_lwr'])\n",
    "        self.Xtis_train = self.Xtis_train.todense()\n",
    "        self.Y_train = self.train_label\n",
    "        \n",
    "        # convert documents and their labels into numpy arrays for test data\n",
    "        self.Xtis_test = self.vectorizer.fit_transform(self.data_test['incidentTitle_lwr'])\n",
    "        self.Xtis_test = self.Xtis_test.todense()\n",
    "        self.Y_test = self.label_test\n",
    "        \n",
    "        # Fit model using Logistic Regression\n",
    "        self.LRti_model = LogisticRegression(C=1).fit(self.Xtis_train, self.Y_train)\n",
    "        \n",
    "        # Print classification report\n",
    "        print classification_report(self.Y_test, self.LRti_model.predict(self.Xtis_test))\n",
    "    \n",
    "    def tfidf_desc_LR(self):\n",
    "        # create vectorizer object\n",
    "        self.vectorizer = TfidfVectorizer(decode_error='replace', stop_words='english', max_features=800)\n",
    "        \n",
    "        # convert documents and their labels into numpy arrays for training data\n",
    "        self.Xtis_train = self.vectorizer.fit_transform(self.train_data['desc_lwr'])\n",
    "        self.Xtis_train = self.Xtis_train.todense()\n",
    "        self.Y_train = self.train_label\n",
    "        \n",
    "        # convert documents and their labels into numpy arrays for test data\n",
    "        self.Xtis_test = self.vectorizer.fit_transform(self.data_test['desc_lwr'])\n",
    "        self.Xtis_test = self.Xtis_test.todense()\n",
    "        self.Y_test = self.label_test\n",
    "        \n",
    "        # Fit model using Logistic Regression\n",
    "        self.LRti_model = LogisticRegression(C=1).fit(self.Xtis_train, self.Y_train)\n",
    "        \n",
    "        # Print classification report\n",
    "        print classification_report(self.Y_test, self.LRti_model.predict(self.Xtis_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature imputation tweaks for Model C\n",
    "\n",
    "We saw that our precision went up by 3% with model b - the next changes to make, model, & test, before diving into NLP will include:\n",
    "- Number of Times Contact Attempted / Number of Times Reached: Use the means to impute missing values for both features.\n",
    "- Add in the length features to see if this ups our model's performance. \n",
    "\n",
    "First we examine what difference the imputation of values for number of times contact attempted / number of times reached makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputation: replace null values with means. \n",
    "\n",
    "# Create a new dataframe so we don't overwrite original data\n",
    "model3_data = data\n",
    "\n",
    "# # Create a variable to store the mean for Number of Times Contact Attempted\n",
    "# avg_nTCA = data.noTimesContactAttempted[data.noTimesContactAttempted.notnull()].mean()\n",
    "# print avg_nTCA\n",
    "# # Create a variable to store the mean for Number of Times Reached\n",
    "# avg_nTR = data.noTimesReached[data.noTimesReached.notnull()].mean()\n",
    "# print avg_nTR\n",
    "\n",
    "# Actually, we would axe this because these variables are too predictive. They happen too late in the pipeline.\n",
    "# Fill missing values for one variable and run \n",
    "# model3_data['noTimesContAtt_imp'] = data.noTimesContactAttempted.fillna(avg_nTCA)\n",
    "# model3_data['noTimesRea_imp'] = data.noTimesRea_imp.fillna(avg_nTR)\n",
    "# model3_data.describe()\n",
    "\n",
    "# Drop the usual features (see extr_var defined above)\n",
    "extr_var2 = ['caseID','incidentTitle','incidentDate','location','desc','category',\\\n",
    "             'latitude','longitude','phoneNo','mostAffctDistr','locAccuracy','forApproval',\\\n",
    "                  'routedStatusOrg','noTimesRouted','locAccurFixer', 'dispatchStatus_orgagreed',\\\n",
    "                  'dispatchOrgCapacity','firstName','lastName','email','closed',\\\n",
    "                  'actionSumm','comment','commentDate', 'approved','noTimesContactAttempted',\\\n",
    "                           'noTimesReached','incTitle_len','desc_len','noTimesContAtt_imp','noTimesRea_imp',\\\n",
    "            'actionSumm_incl', 'email_incl','urgent', 'comment_incl', 'firstName_incl', 'lastName_incl', 'verified']\n",
    "model3a = model3_data.drop(extr_var2, axis=1)\n",
    "# Check that the features included are correct\n",
    "model3a.head()\n",
    "\n",
    "# Normalizing features: because we now have actual continuous variables & dummy variables, we need to normalize so they're\n",
    "# on the same scale. \n",
    "\n",
    "# Import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create dataframe with the variables we want to normalize\n",
    "# model3_datatn = model3_data[['noTimesContAtt_imp','noTimesRea_imp']]\n",
    "# model3_datatn.head()\n",
    "\n",
    "# # Normalize the data\n",
    "# fit_data = StandardScaler().fit_transform(model3_datatn)\n",
    "# model3_data_n = pd.DataFrame(fit_data, columns=model3_datatn.columns)\n",
    "# model3_data_n.info()\n",
    "\n",
    "# Append the normalized features to our model 3 dataframe\n",
    "# Create for loop to go through each column in the normalized features dataframe\n",
    "# for col in model3_data_n.columns:\n",
    "#     # Append each column to the model 3 dataframe\n",
    "#     model3a[col] = model3_data_n[col]\n",
    "# # Check that the data has been correctly appended\n",
    "# model3a.head()\n",
    "# model3a.info()\n",
    "\n",
    "model_c = datasample(model3a)\n",
    "model_c.sss()\n",
    "model_c.create_sample(301)\n",
    "model_c.create_baldata()\n",
    "\n",
    "# Try logistic regression \n",
    "model_c.get_LRreport()\n",
    "\n",
    "# Visualize the feature importance. \n",
    "model_c.get_LRfeatimport()\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=4, n_estimators=10, max_features=2)\n",
    "model_c.get_modelreport(rfc)\n",
    "\n",
    "svc = SVC(kernel='rbf',C=1)\n",
    "model_c.get_modelreport(svc)\n",
    "\n",
    "svc_lin = SVC(kernel='linear',C=1)\n",
    "model_c.get_modelreport(svc_lin)\n",
    "\n",
    "model_c.c_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP stuff\n",
    "- Other feature: create a new column called text length (df['string']=df.[whichevercolumnisthetext].apply lambdax: len(x))\n",
    "- do both for title + description - done\n",
    "- check for feature importance - done\n",
    "- don't normalize the binary column (if normalize, that implies it's a continuous variable) - done\n",
    "- normalize by length - done \n",
    "\n",
    "- NLP:\n",
    "> - tolower() -> converts all text to lowercase. x.tolower() - done\n",
    "> - [get rid of punctuation]\n",
    "> - ham-spam lab - count vectorizer, tf-idf vectorizer\n",
    "> - tf-idf vectorizer: easiest way to get the matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
