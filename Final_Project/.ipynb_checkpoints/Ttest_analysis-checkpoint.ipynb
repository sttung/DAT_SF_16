{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploration of our Number of Times Contact Attempted & Number of Times Reached variables' missing values. \n",
    "\n",
    "# Exploration: is our data on the number of times contact was attempted randomly missing?\n",
    "# & is data on the number of times someone was reached randomly missing?\n",
    "# How to check: are there statistically significant differences between features, if we split our data on missing/non-missing?\n",
    "\n",
    "# (1) Number of Times Contact Attempted feature: \n",
    "# Create dummies to separate out noTimesContactAttempted missing / non-missing observations\n",
    "model2c = data\n",
    "model2c['noTimesContAtt_incl'] = data.noTimesContactAttempted.notnull()==True\n",
    "model2c['noTimesReached_incl'] = data.noTimesReached.notnull()==True\n",
    "# model2c.head()\n",
    "\n",
    "# Import ttest from SciPy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define the 2 groups to be compared for number of times contact attempted\n",
    "noTCA_1 = model2c[model2c['noTimesContAtt_incl']==True]\n",
    "noTCA_2 = model2c[model2c['noTimesContAtt_incl']==False]\n",
    "\n",
    "# Run a t-test on verified (even though it's a dummy, we can look at the means to gauge if more reports were verif or not)\n",
    "\n",
    "# Define a function that runs ttests on a couple different features to identify any stat signif differences\n",
    "def tt_feat1(features):\n",
    "    print\n",
    "    print \"t-test results for number of times contact attempted\"\n",
    "    # Loop through the list of features we want to test for stat signif differences between\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTCA_1[x], noTCA_2[x])\n",
    "        \n",
    "tt_feat1(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])      \n",
    "\n",
    "# (2) Number of Times Contact Reached: \n",
    "# Define the 2 groups to be compared for number of times reached\n",
    "noTR_1 = model2c[model2c['noTimesReached_incl']==True]\n",
    "noTR_2 = model2c[model2c['noTimesReached_incl']==False]\n",
    "\n",
    "def tt_feat2(features):\n",
    "    print\n",
    "    print \"t-test results for number of times reached:\"\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTR_1[x], noTR_2[x])    \n",
    "        \n",
    "tt_feat2(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])\n",
    "\n",
    "# Next steps:\n",
    "# Use average\n",
    "# then can try distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is old code used to run the first logistic regression model.\n",
    "# Replaced this with a function, because we're essentially running the same code each time we build a model.\n",
    "# Using a function makes the code much easier to follow as long as the function is well-explained / commented. \n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m1data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train = balanced_m1data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train = balanced_m1data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test = model1_data.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test = model1_data['actionTaken'].iloc[test]\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr1 = model.fit(data_train, label_train)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test,model_lr1.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for model 2. Replaced by a function.\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m2data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train2 = balanced_m2data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train2 = balanced_m2data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test2 = model2a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test2 = model2a['actionTaken'].iloc[test]\n",
    "    \n",
    "# Run logistic regression on our training data\n",
    "model_lr2 = model2.fit(data_train2, label_train2)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test2,model_lr2.predict(data_test2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
