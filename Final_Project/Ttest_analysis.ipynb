{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploration of our Number of Times Contact Attempted & Number of Times Reached variables' missing values. \n",
    "\n",
    "# Exploration: is our data on the number of times contact was attempted randomly missing?\n",
    "# & is data on the number of times someone was reached randomly missing?\n",
    "# How to check: are there statistically significant differences between features, if we split our data on missing/non-missing?\n",
    "\n",
    "# (1) Number of Times Contact Attempted feature: \n",
    "# Create dummies to separate out noTimesContactAttempted missing / non-missing observations\n",
    "model2c = data\n",
    "model2c['noTimesContAtt_incl'] = data.noTimesContactAttempted.notnull()==True\n",
    "model2c['noTimesReached_incl'] = data.noTimesReached.notnull()==True\n",
    "# model2c.head()\n",
    "\n",
    "# Import ttest from SciPy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define the 2 groups to be compared for number of times contact attempted\n",
    "noTCA_1 = model2c[model2c['noTimesContAtt_incl']==True]\n",
    "noTCA_2 = model2c[model2c['noTimesContAtt_incl']==False]\n",
    "\n",
    "# Run a t-test on verified (even though it's a dummy, we can look at the means to gauge if more reports were verif or not)\n",
    "\n",
    "# Define a function that runs ttests on a couple different features to identify any stat signif differences\n",
    "def tt_feat1(features):\n",
    "    print\n",
    "    print \"t-test results for number of times contact attempted\"\n",
    "    # Loop through the list of features we want to test for stat signif differences between\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTCA_1[x], noTCA_2[x])\n",
    "        \n",
    "tt_feat1(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])      \n",
    "\n",
    "# (2) Number of Times Contact Reached: \n",
    "# Define the 2 groups to be compared for number of times reached\n",
    "noTR_1 = model2c[model2c['noTimesReached_incl']==True]\n",
    "noTR_2 = model2c[model2c['noTimesReached_incl']==False]\n",
    "\n",
    "def tt_feat2(features):\n",
    "    print\n",
    "    print \"t-test results for number of times reached:\"\n",
    "    for x in features: \n",
    "        print \n",
    "        print x\n",
    "        print \n",
    "        print ttest_ind(noTR_1[x], noTR_2[x])    \n",
    "        \n",
    "tt_feat2(['actionTaken','urgent', 'actionable', 'closed', 'urgent', \\\n",
    "         'verified', 'phoneNo_incl', 'locAccurFixer_incl'])\n",
    "\n",
    "# Next steps:\n",
    "# Use average\n",
    "# then can try distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO UPDATE: CREATE A CLASS SO THAT BALANCED_M1DATA CAN BE MODIFIED AND USED BY THE BAL_MODELLR FUNCTION! \n",
    "# def b_dataset:\n",
    "#     # Note that we should have 1634 observations for this set.\n",
    "#     neg = model1_data[model1_data['actionTaken'] == 0]\n",
    "#     print neg.info()\n",
    "\n",
    "#     # Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "#     neg_samp = neg.sample(401)\n",
    "\n",
    "#     # Construct the positive (action taken on reports) subsample. \n",
    "#     pos = model1_data[model1_data['actionTaken'] == 1]\n",
    "\n",
    "#     # Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "#     balanced_m1data = pd.concat([neg_samp,pos])\n",
    "#     balanced_m1data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is old code used to create a training dataset. This \n",
    "\n",
    "# Note that only about 20% of our reports had action taken on them. \n",
    "# To make sure that our model can accurately predict reports where action was taken, we want to create a more balanced dataset.\n",
    "# We create one that is 50/50 \n",
    "\n",
    "# As a first step - separate out the data into cases where reports had action taken (a/t), or no a/t. \n",
    "\n",
    "# Create a variable that stores cases where action is not taken on a report.\n",
    "# Note that we should have 1634 observations for this set.\n",
    "neg = model1_data[model1_data['actionTaken'] == 0]\n",
    "neg.info()\n",
    "\n",
    "# Sample 401 observations of this subsample. We select 401 to match the number of reports where action was taken. \n",
    "neg_samp = neg.sample(401)\n",
    "\n",
    "# Construct the positive (action taken on reports) subsample. \n",
    "pos = model1_data[model1_data['actionTaken'] == 1]\n",
    "\n",
    "# Combine both samples: sampled no action taken reports, entire set of action taken reports.\n",
    "balanced_m1data = pd.concat([neg_samp,pos])\n",
    "balanced_m1data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logistic regression model\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Create a function to test out our features using a logistic regression model.\n",
    "# This will allow us to more quickly add features and trims the amount of code we need to use each time. \n",
    "def bal_modelLR(baldf, modeldf, model):\n",
    "    # Create our training & test datasets \n",
    "    for train,test in ShuffleSplit(len(baldf), n_iter=1, test_size=0.25,random_state=0):\n",
    "        # Drop label from training data\n",
    "        data_train = baldf.iloc[train].drop(['actionTaken'], axis=1)\n",
    "        # Create label set \n",
    "        label_train = baldf['actionTaken'].iloc[train]\n",
    "        # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "        data_test = modeldf.iloc[test].drop(['actionTaken'], axis=1)\n",
    "        label_test = modeldf['actionTaken'].iloc[test]\n",
    "\n",
    "    # Run logistic regression on our training data\n",
    "    model_lr = model.fit(data_train, label_train)\n",
    "\n",
    "    # Print out details on precision, recall for our model. \n",
    "    print classification_report(label_test,model_lr.predict(data_test))\n",
    "    \n",
    "    # Print out the coefficients\n",
    "    model.coef[0]\n",
    "    print zip(model.coef_[0],modeldf.drop('actionTaken',axis=1))\n",
    "\n",
    "bal_modelLR(balanced_m1data, model1_data,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is old code used to run the first logistic regression model.\n",
    "# Replaced this with a function, because we're essentially running the same code each time we build a model.\n",
    "# Using a function makes the code much easier to follow as long as the function is well-explained / commented. \n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m1data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train = balanced_m1data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train = balanced_m1data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test = model1_data.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test = model1_data['actionTaken'].iloc[test]\n",
    "\n",
    "# Run logistic regression on our training data\n",
    "model_lr1 = model.fit(data_train, label_train)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test,model_lr1.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old code for model 2. Replaced by a function.\n",
    "\n",
    "# Create our training & test datasets \n",
    "for train,test in ShuffleSplit(len(balanced_m2data), n_iter=1, test_size=0.25,random_state=0):\n",
    "    # Drop label from training data\n",
    "    data_train2 = balanced_m2data.iloc[train].drop(['actionTaken'], axis=1)\n",
    "    # Create label set \n",
    "    label_train2 = balanced_m2data['actionTaken'].iloc[train]\n",
    "    # Define our test data (features), label. Note we have to drop malignant since we're generating this off the orig data. \n",
    "    data_test2 = model2a.iloc[test].drop(['actionTaken'], axis=1)\n",
    "    label_test2 = model2a['actionTaken'].iloc[test]\n",
    "    \n",
    "# Run logistic regression on our training data\n",
    "model_lr2 = model2.fit(data_train2, label_train2)\n",
    "\n",
    "# Print out details on precision, recall for our model. \n",
    "print classification_report(label_test2,model_lr2.predict(data_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
